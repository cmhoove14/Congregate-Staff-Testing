---
title: "Optimizing COVID-19 screening for shift-workers to reduce introduction of disease from the community"
subtitle: "Methods and results"
author: "Chris Hoover et al"
date: "5/20/2021"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo    = FALSE,
                      include = FALSE,
                      warning = FALSE,
                      message = FALSE)

library(tidyverse)
library(patchwork)
source(here::here("R/Utils.R"))
source(here::here("R/Sim_Functions.R"))

```

## To-dos  
* Add non-compliance with testing regimen 
* Make self-isolation a scenario  
  - Grid of self-isolation to testing compliance?  
* Format for analytic results, followed by grid search of scenarios, followed by sensitivity analyses of optimal scenario subjected to: non-compliance, leaky cohort  

## Staff work and testing schedules  
[Nick describes CDCR data, staffing and testing schedules]

## Model framework and parameterization for SARS-CoV2  
Building on previous work investigating the effects of non-pharmaceutical interventions [CITE](https://doi.org/10.1073/pnas.1616438114) and testing [CITE](https://doi.org/10.1126/sciadv.abd5393) on the transmission of infectious diseases, individual contributions to SARS-CoV2 transmission through time are modeled from an infectiousness profile, $\beta_t$, generated from key biological parameters of the virus that determine the distribution of infectiousness over time. A triangle distribution is used to model $\beta_t$, with infectiousness beginning after the latent period, ending after the duration of the infectious period, and peaking at some point in between ($a=t_{latent}$, $b=t_{tot}=t_{infectious}+t_{latent}$, $c=t_{peak}$, and a<c<b ; Fig 1a). 

The viral dynamics of SARS-CoV2 make control efforts challenging, as asymptomatic and presymptomatic transmission caused by high infectiousness in the absence of symptoms are common [CITE](https://wwwnc.cdc.gov/eid/article/27/4/20-4576_article), [CITE](https://science.sciencemag.org/content/368/6490/489). In terms of the infectiousness profile for SARS-CoV2, this means that peak infectiousness $t_{peak}$ tends to coincide with the onset of symptoms (for cases that are symptomatic), but occurs after completion of the latent period (i.e. $t_{peak}\approx t_{incubation}$>$t_{latent}$). The expected number of new cases generated at time $t$ is thus $r_{t}=\mathcal{R}\beta_{t}$, where $\mathcal{R}$ is the effective reproduction number interpreted as the expected number of cases generated by a new case over the duration of the infectious period. The model therefore assumes that new cases are most likely to be generated around the time of peak infectiousness, $t_{peak}$. Table 1 lists the distributions of $t_{incubation}$, $t_{latent}$, and $t_{infectious}$ used here.

In the presence of interventions that isolate infectious individuals prior to $t_{tot}$, e.g. through contact tracing, self-isolation following the onset of symptoms, or testing, the effect of isolation on $\mathcal{R}$ can be directly estimated from the time to isolation as $\mathcal{R}_{iso}=\mathcal{R}\big(1-\int_{t_{iso}}^{t_{tot}}\beta_tdt\big)$, where $t_{iso}$ is the time at which isolation occurs. Reducing $\mathcal{R}_{iso}$ via improved contact tracing or more frequent testing can thus be envisioned as removing a larger slice from the overall infectiousness triangle by reducing $t_{iso}$ (Fig 1a). The size of the slice removed is dependent on the shape of the overall triangle distribution, which is primarily determined by $t_{tot}$ and $t_{peak}$, and the location of $t_{iso}$ in relation to $t_{peak}$. For instance, if $t_{iso}>t_{peak}$, then the proportional reduction in $\mathcal{R}$ can be estimated as $\frac{2}{t_{infectious}(t_{tot}-t_{peak})}\int_{t_{iso}}^{t_{tot}}(t_{tot}-t)dt$. Figure 1b shows the relationship between $\mathcal{R}_{iso}$ and $t_{iso}$ is sigmoidal, implying earlier isolation is incrementally more effective and the benefits of isolation level off later in the infectious period. Other interventions that reduce $\mathcal{R}$ across all levels of infectiousness such as wearing a mask or reducing the contact rate between infectious and susceptible individuals can also be accommodated simply by multiplying $\mathcal{R}$ by a constant.

```{r include = FALSE, eval = FALSE}
# Text to add re: if t_iso is less than t_peak: more complicated integral

, whereas if $t_{iso}<t_{peak}$, reduction in $R$ is: $\frac{2(t_{tot}-t_{iso})}{t_{infectious}(t_{tot}-t_{peak})}+\frac{2}{t_{infectious}}+\frac{2(t_{iso})}{t_{infectious}(t_{peak}-t_{latent})}$
```


```{r tri_examp, echo = FALSE}
# Triangle schematic -----------------
base_R   <- 1
t_latent <- 3.5
t_peak   <- 5
t_infect <- 8.5
t_tot    <- t_latent + (t_peak-t_latent) + t_infect
dt       <- 0.1

tri <- infectious_profile(t_latent, t_peak, t_infect, dt)

tri_R <- tri*base_R

t_iso <- t_peak + 2

d_iso <- dtriangle(x = t_iso, 
                   a = t_latent,
                   b = t_tot,
                   c = t_peak)

base_R_iso <- base_R * (1-d_iso*(t_tot-t_iso)/2) # reduction equal to 1 minus area of triangle removed 

# Plot with infectiousness removed
tri_examp_plot <- tibble(
  d_inf = seq(0, t_tot, by = dt),
  Rt    = tri_R
  ) %>% 
  ggplot() +
  geom_line(aes(x = d_inf, y = Rt)) + 
  geom_polygon(
    data    = tibble(x = c(t_iso, t_iso,        t_tot),
                     y = c(0    , d_iso*base_R, 0)),
    mapping = aes(x = x, y = y),
    fill    = "grey50",
    col     = "red"
    ) +
  scale_x_continuous(breaks = seq(0,14,2)) +
  theme_classic() +
  theme(axis.text = element_text(size = 11),
        axis.title = element_text(size = 13)) +
  labs(x   = "Days since infection",
       y   = expression(beta[t]),
       tag = "A")

# R_iso as function of t_iso -----------------
t_isos <- seq(3,14,0.1)
R_isos <- sapply(t_isos, function(t){
  R_iso(t_latent, t_peak, t_infect, t, base_R)
})
 
r_iso_t_iso_plot <- tibble(
  t_isos = t_isos,
  R_isos = sapply(t_isos, function(t){
    R_iso(t_latent, t_peak, t_infect, t, base_R)
  })
) %>% 
  ggplot() +
    geom_line(aes(x = t_isos, y = R_isos)) +
    geom_point(data = tibble(x = t_iso, y = base_R_iso),
               aes(x = x, y = y),
               col = "red", size = 2) +
    theme_classic() +
    theme(axis.text = element_text(size = 11),
          axis.title = element_text(size = 13)) +
    scale_x_continuous(breaks = seq(1,15,by=2)) +
    labs(x = expression(t[iso]),
         y = expression(R[iso]),
         tag = "B")
```

```{r save_triangle, include = FALSE, echo = FALSE}
tri_examp_plot

ggsave(here::here("Plots/triangle_example.png"),
       width = 6, height = 3, units = "in")

tri_examp_plot2 <- tibble(
  d_inf = seq(0, t_tot, by = dt),
  Rt    = tri_R
  ) %>% 
  ggplot() +
  geom_line(aes(x = d_inf, y = Rt)) + 
  scale_x_continuous(breaks = seq(0,14,2)) +
  theme_classic() +
  theme(axis.text = element_text(size = 11),
        axis.title = element_text(size = 13)) +
  labs(x   = "Days since infection",
       y   = expression(beta[t]),
       tag = "A")

ggsave(here::here("Plots/triangle_example_no_iso.png"),
       width = 6, height = 3, units = "in")

```

```{r p_tiso, echo = FALSE, fig.width=6, fig.height=4, fig.cap="Probability of going $t$ days without being tested given constant testing frequencies"}
test_freqs <- 1/c(1,3,5,7,10,14)

fd_grid <- as.data.frame(expand.grid("freqs" = test_freqs, 
                                     "days"  = c(1:14),
                                     "delay" = c(0:2)))

fd_grid$p_t_iso = apply(fd_grid,1,function(x){
  1-(1-1/(x[1]^-1+x[3]))^x[2]
})

d_labs <- paste0("d = ", c(0:2))
names(d_labs) <- c(0:2)

p_iso_t_freq_plot <- fd_grid %>% 
  ggplot(aes(x = days, y = p_t_iso, col = as.factor(freqs))) +
    geom_line() +
    theme_classic() +
    theme(axis.text = element_text(size = 11),
          axis.title = element_text(size = 13),
          strip.background = element_blank(),
          strip.text = element_text(face = "italic",
                                    size = 11)) +
    scale_color_manual(breaks = test_freqs,
                       values = c("#c57c3c", "#6588cd", "#7fa83c", "#ab62c0", "#5ca576", "#ca5670"),
                       labels = c("1", "3", "5", "7", "10", "14")) +
    scale_x_continuous(breaks = c(1,3,5,7,9,11,13)) +
    facet_wrap("delay", nrow = 3,
               labeller = labeller(delay = d_labs)) +
    ylim(c(0,1)) +
    labs(x = expression(tau~days),
         y = expression(P(t[iso]<=tau)),
         col = "Testing\n Frequency",
         tag = "D")
```

```{r test_freq_r_red_par_uncertainty_distn, echo = FALSE}
n_sims <- 100
R_iso_test_freqs <- bind_rows(lapply(test_freqs, function(f){
  # Draw from distributions for key time periods
  inf_pars <- sim_inf_pars(n_sims)
  
  t_inc_samps <- inf_pars[,1]
  t_lnt_samps <- inf_pars[,2]
  t_inf_samps <- inf_pars[,3]
  
  R_iso_d0 <- sapply(1:n_sims, function(i){
    R_iso_f(
      t_latent     = t_lnt_samps[i], 
      t_peak       = t_inc_samps[i], 
      t_infectious = t_inf_samps[i], 
      t_freq       = f, 
      dt           = dt, 
      d            = 0,
      R            = base_R
    )
  })
  
  R_iso_d1 <- sapply(1:n_sims, function(i){
    R_iso_f(
      t_latent     = t_lnt_samps[i], 
      t_peak       = t_inc_samps[i], 
      t_infectious = t_inf_samps[i], 
      t_freq       = f, 
      dt           = dt, 
      d            = 1,
      R            = base_R
    )
  })

  R_iso_d2 <- sapply(1:n_sims, function(i){
    R_iso_f(
      t_latent     = t_lnt_samps[i], 
      t_peak       = t_inc_samps[i], 
      t_infectious = t_inf_samps[i], 
      t_freq       = f, 
      dt           = dt, 
      d            = 2,
      R            = base_R
    )
  })
    
  return(tibble(R_isos = c(R_iso_d0, R_iso_d1, R_iso_d2),
                delay  = c(rep(0, n_sims), rep(1, n_sims), rep(2, n_sims)),
                t_freq = f))
}))

exp_R_iso_t_freq_plot <- R_iso_test_freqs %>% 
  ggplot(aes(y = R_isos, x = as.factor(t_freq), col = as.factor(delay))) +
    geom_boxplot() +
    #geom_point(alpha = 0.5, size = 0.3, position = position_dodge2(0.5)) +
    # stat_summary(fun = "median", #fun.min = "q_25", fun.max = "q_75",
    #              position = position_dodge2(1),
    #              size = 0.2) +
    scale_x_discrete(labels = rev(1/test_freqs)) +
    scale_color_manual(values = c("#ff7eca", "#00b7fe", "#4e4b8b")) +
    ylim(c(0, base_R)) +
    theme_classic() +
    theme(axis.text = element_text(size = 11),
        axis.title = element_text(size = 13)) +
    labs(x = "Testing frequency (days)",
         y = expression(R[iso]),
         col = expression(italic(d)),
         tag = "C")
```


```{r test_freq_r_red, eval = FALSE}
R_iso_d0s <- sapply(test_freqs, function(f){
    R_iso_f(
      t_latent     = t_latent, 
      t_peak       = t_peak, 
      t_infectious = t_infect, 
      t_freq       = f, 
      dt           = dt, 
      d            = 0,
      R            = base_R
    )
  })

R_iso_d1s <- sapply(test_freqs, function(f){
    R_iso_f(
      t_latent     = t_latent, 
      t_peak       = t_peak, 
      t_infectious = t_infect, 
      t_freq       = f, 
      dt           = dt, 
      d            = 1,
      R            = base_R
    )
  })

R_iso_d2s <- sapply(test_freqs, function(f){
    R_iso_f(
      t_latent     = t_latent, 
      t_peak       = t_peak, 
      t_infectious = t_infect, 
      t_freq       = f, 
      dt           = dt, 
      d            = 2,
      R            = base_R
    )
  })

R_iso_test_freqs2 <- tibble(R_isos = c(R_iso_d0s, R_iso_d1s, R_iso_d2s),
                           delay  = c(rep(0, length(test_freqs)), 
                                      rep(1, length(test_freqs)), 
                                      rep(2, length(test_freqs))),
                           t_freq = rep(test_freqs, times = 3))

exp_R_iso_t_freq_plot2 <- R_iso_test_freqs2 %>% 
  ggplot(aes(y = R_isos, x = as.factor(t_freq), col = as.factor(delay))) +
    geom_violin(alpha = 0.7) +
    #geom_point(alpha = 0.5, size = 0.3, position = position_dodge2(0.5)) +
    stat_summary(fun = "median", #fun.min = "q_25", fun.max = "q_75",
                 position = position_dodge2(1),
                 size = 0.2) +
    scale_x_discrete(labels = rev(1/test_freqs)) +
    ylim(c(0, base_R)) +
    theme_classic() +
    theme(axis.text = element_text(size = 11),
        axis.title = element_text(size = 13)) +
    labs(x = "Testing frequency (days)",
         y = expression(R[iso]),
         col = expression(italic(d)),
         tag = "C")
```

**Table 1**: Distributions and parameter values used in analytic framework and model simulations  
```{r tab1, include = TRUE}
data.frame("Parameter" = c("Incubation Period", "Latent Period", "Infectious Period", 
                           "Community Prevalence", "R"),
           "Distribution" = c("Lognormal(1.63, 0.5)", "t_inc + Uniform(-2,0)", "(t_peak-t_latent) + Uniform(6.5,9.5)",
                              "[0.1%, 0.5%, 1%]", "[0.5, 1.0, 1.5]"),
           "Source" = rep("CITE", 5)) %>% 
  knitr::kable()
```

Assuming testing is independent of symptoms, known contacts, and other reasons for explicitly seeking testing, the probability of going $t$ days without being tested from the testing frequency, $f$, can be estimated as $(1-f)^t$ and the probability that $t_{iso}\leq\tau$ as $\mathrm{P}(t_{iso}\leq\tau|f)=1-(1-f)^\tau$, assuming isolation occurs immediately after testing. Given substantial turnaround times between testing and isolation, particularly when relying on PCR-based tests, the delay between testing and isolation, $d$, can also be incorporated as: $\mathrm{P}(t_{iso}=\tau|f,d)=1-\Big(1-\frac{1}{f^{-1}+d}\Big)^\tau$, where $f^{-1}$ is simply the average number of days between tests. Figure 1d shows that such delays have a detrimental effect that is greater than additive on the probability of achieving prompt isolation. For example, with a daily testing frequency and no delay, $\mathrm{P}(t_{iso}=\tau|f=1,d=0)=1$. However, increasing the delay to 1 or 2 days leads to $\mathrm{P}(t_{iso}=\tau|f=1,d=1)=$ `r fd_grid[which(fd_grid$freqs == 1 & fd_grid$delay == 1 & fd_grid$days == 3),"p_t_iso"]` and $\mathrm{P}(t_{iso}=\tau|f=1,d=2)=$ `r round(fd_grid[which(fd_grid$freqs == 1 & fd_grid$delay == 2 & fd_grid$days == 3),"p_t_iso"], 3)`, respectively. 

Testing frequency and delay can also be incorporated into estimation of $\mathcal{R}_{iso}$, essentially as the infectiousness on day $\tau$ weighted by the probability of remaining un-isolated on day $\tau$ :  
$$\mathbb{E}[\mathcal{R}_{iso}|\beta_t,f,d]=\mathcal{R}\int_{\tau=t_{latent}}^{t_{infectious}}\beta_\tau\Big(1-\frac{1}{f^{-1}+d}\Big)^\tau d\tau$$

Figure 1c shows distributions of $R_{iso}$ derived from 100 random draws sampling from uncertainty in the SARS-CoV2 latent, peak, and total infectious periods, across test frequencies ranging from daily ($f=1$) to biweekly ($f=14$) and test delays from 0 to 2 days. These results again reiterate the importance of reducing test delays, as $\mathcal{R}_{iso}$ is approximately the same when testing every day ($f=1$) with a two-day turnaround time for test results ($d=2$) vs testing every three days ($f=3$) with immediate test results ($d=0$) (fig 1c, median $\mathcal{R}_{iso}(d=0,f=3)=$ `r round(median(R_iso_test_freqs$R_isos[which(R_iso_test_freqs$delay == 0 & R_iso_test_freqs$t_freq == test_freqs[2])]), digits = 2)` and $\mathcal{R}_{iso}(d=2,f=1)=$ `r round(median(R_iso_test_freqs$R_isos[which(R_iso_test_freqs$delay == 2 & R_iso_test_freqs$t_freq == test_freqs[1])]), digits = 2)`, respectively).

```{r mod_schematic_plot, echo = FALSE, include = TRUE, fig.width=8, fig.height=6, fig.cap="**Figure 1. Model framework and analytic results.** A) Example infectiousness profile for $\\mathcal{R}=1$, $t_{latent}=3.5$, $t_{peak}=5$, $t_{infectious}=10$, with shaded area demonstrating infectiousness slice removed if $t_{iso}=7$, leading to $\\mathcal{R}_{iso}=0.5$. B) $\\mathcal{R}_{iso}$ as a function of $t_{iso}$ with same parameters as in A and point indicating scenario depicted in A. C)  Boxplots showing distributions of $\\mathbb{E}[\\mathcal{R}_{iso}|\\beta_t,f,d]$ as a function of testing frequency, $f$, and test delay, $d$, incorporating uncertainty in $t_{latent}$, $t_{peak}$, and $t_{infectious}$ by drawing $n=100$ parameter sets for each, with baseline $\\mathcal{R}=1$. Boxplots indicate median, interquartile range, and full range of values of $\\mathbb{E}[\\mathcal{R}_{iso}|\\beta_t,f,d]$. D) Relationship between testing frequency, $f$, test delay, $d$, and probability isolation occurs by day $t$, i.e. $t_{iso}\\leq t$, demonstrating that delays in testing substantially reduce the probability of prompt isolation, particularly in more frequent testing scenarios."}

# Four panels with patchwork
(tri_examp_plot / r_iso_t_iso_plot / exp_R_iso_t_freq_plot) | p_iso_t_freq_plot 

ggsave(here::here("Plots/Framework_Analysis.png"),
       units = "in", height = 6, width = 7)
```


## Individual-based model simulations   

### Model setup (can probably move a lot of this to a supplement?)  
To incorporate staff schedules and expand the modeling framework above to a facility-level setting, we next describe the development and simulation of individual based microsimulations. In a modeled facility, $n$ staff are assigned a work schedule that determines time frames when they are in the facility and interacting with facility residents and other staff working at the same time. We denote $\mathcal{W}(w_{it})$ as an indicator function for whether staff member $i$ is working at the facility on day $t$. In addition to their work schedule, all staff are assigned a testing schedule, encoded by function $\mathcal{T}(w_{it})$, with different testing schedules discussed further below. The model is simulated at an 8-hour time step, with each time step corresponding to a work shift as described further below.  

Staff move through susceptible (S), exposed (E), infected (I), and recovered (R) states, with the infected state corresponding to time when $\beta_{it}>0$. Parameters for newly exposed staff are drawn to determine $t_{latent}$, $t_{incubation}$, and $t_{infectious}$, from which an infectiousness profile, $\beta_{it}$ is generated. Tested staff produce a positive result if $\beta_{it}>0$ and $\mathcal{T}(w_{it})=1$, at which time they enter a quarantined (Q) state immediately if $d=0$, or first enter a tested (T) state before Q if there is a delay between test administration and the test result. Staff in state Q have $\mathcal{W}(w_{it})=0$ for 10 days and have $\mathcal{T}(w_{it})=0$ for 90 days following a positive result.

Assuming constant $\mathcal{R}$ across all individuals, the expected number of cases produced in the facility on day $t$ by individual $i$ is $r_{it}=\mathcal{R}\beta_{it}\mathcal{W}(w_{it})$. Staff may acquire infection from the community or workplace, therefore we denote two separate forces of infection, $\lambda^{comm}=$ and $\lambda^{work}_{it}=\frac{\sum_{i=1}^nr_{it}}{n}$. Infection for each individual is simulated at each time step by subjecting each staff member to a bernoulli trial with $p_{it}=\mathcal{W}(w_{it})\lambda^{work}_{it}+\big(1-\mathcal{W}(w_{it})\big)\lambda^{comm}$. The expected number of infections in the facility generated by staff is estimated from each simulation as: $\mathcal{I}^{tot}_{sim}=\sum_{t=1}^{t_{sim}}\sum_{i=1}^n\mathcal{W}(w_{it})r_{it}$.  

### Staffing and testing strategies  
[One sentence summary of test/staffing strategies observed in CDCR data]. Generation of work schedules in simulations was informed by observed CDCR schedules by sampling one of the consecutive 4-day sequences of work days shown in Fig 1 and a regular shift (morning, evening, night) for each worker. A fifth shift was then added to each worker's weekly schedule by randomly sampling from all other potential shifts. Two testing strategies were considered. Under a random testing strategy, testing for each worker occurs at random during their five shifts depending on the frequency (i.e. with $f=2$, workers would be tested during two of the five shifts, chosen at random each week). Under a systematic testing strategy, each worker is always tested on the same day(s) of their shift each week. For $f=1$, systematic testing always occurs on the first of the regular 4-day work days; for $f=2$, systematic testing always occurs on the first and third days; and for $f=4$, testing occurs on each of the regular 4-day work days. 

We assume rapid tests in which the test result is known immediately after the test is conducted ($d=0$) are used and further assume that all tests conducted when $\beta_{it}>0$ return a positive result. The total number of tests conducted in each simulation is recorded as: $\mathcal{T}^{tot}_{sim}=\sum_{t=1}^{t_{sim}}\sum_{i=1}^n\mathcal{T}(w_{it})$. Combined with the expected number of cases in the simulation, we estimate the incremental test effectiveness ratio (ITER) as: $\mathrm{ITER}=\frac{\mathcal{T}^{tot}_{sim}}{\mathcal{I}^{tot}_{sim}-\mathcal{I}^{tot}_{ref}}$, where $\mathcal{I}^{tot}_{ref}$ is the number of infections in a reference scenario with no testing. The ITER can be interpreted as the number of tests needed to prevent one infection in the simulation scenario being evaluated.   

## Simulation Results  
```{r load_sim_results}
load(here::here("data/sim_results_processed.RData"))

sims_sum_ggplot <- sims_sum_ggplot %>% 
  mutate(commprev = `Community Prevalence`)
```

Systematic testing strategies were found to consistently outperform random testing strategies in terms of preventing infections within simulated facilities. Figure 2 shows a comparison of the number of infections generated ($\mathcal{I}^{tot}_{sim}$) when implementing a random vs systematic testing strategy across testing frequencies, community prevalences, and within-facility $\mathcal{R}$. In the highest transmission scenario ($CP=1\%, \mathcal{R}=1.5$), testing randomly once per week resulted in a median $\mathcal{I}^{tot}_{sim}=$ `r round(sims_sum2 %>% filter(testfreq == 1, work_sched == "leaky", R == 1.5, lambda == lambda3, testsys == "random") %>% pull(totcases_1), 2)` (IQR `r round(sims_sum2 %>% filter(testfreq == 1, work_sched == "leaky", R == 1.5, lambda == lambda3, testsys == "random") %>% pull(totcases_2), 2)` - `r round(sims_sum2 %>% filter(testfreq == 1, work_sched == "leaky", R == 1.5, lambda == lambda3, testsys == "random") %>% pull(totcases_3), 2)`), whereas testing systematically on the first day of the work week resulted in $\mathcal{I}^{tot}_{sim}=$ `r round(sims_sum2 %>% filter(testfreq == 1, work_sched == "leaky", R == 1.5, lambda == lambda3, testsys == "systematic") %>% pull(totcases_1), 2)` (IQR `r round(sims_sum2 %>% filter(testfreq == 1, work_sched == "leaky", R == 1.5, lambda == lambda3, testsys == "systematic") %>% pull(totcases_2), 2)` - `r round(sims_sum2 %>% filter(testfreq == 1, work_sched == "leaky", R == 1.5, lambda == lambda3, testsys == "systematic") %>% pull(totcases_3), 2)`; Fig 2, right panel in pink). 

The horizontal gray line in figure 2 demonstrates a potential threshold number of infections to avoid exceeding at $\mathcal{I}^{tot}_{sim}=20$. Implementing a systematic--rather than random--testing strategy can be sufficient to prevent $\mathcal{I}^{tot}_{sim}$ from exceeding such a threshold without changing the frequency in many transmission scenarios, though in the highest transmission scenarios, greater than twice-weekly testing may be needed.

```{r sim_results, include = TRUE, fig.width=6, fig.height=4, fig.cap="**Figure 2. Number of expected infections in a facility from model simulations comparing random and systematic testing strategies across transmission scenarios and test frequencies**. Systematic testing strategies ([triangles]) prevent more infections than random strategies ([circles]) across all transmission scenarios and test frequencies. The horizontal gray line serves as a reference to assess the testing frequency needed to maintain $\\mathcal{I}^{tot}_{sim}\\leq20$ across different transmission scenarios. Error bars represent the interquartile range of $\\mathcal{I}^{tot}_{sim}$ derived from 100 simulations per scenario."}

sims_sum_ggplot %>% 
  #filter(measure %in% c("Transmissions avoided/\n1000 tests", "Transmissions"), work_sched == "leaky") %>% 
  filter(measure == "Transmissions", work_sched == "leaky") %>% 
  ggplot(aes(x = `Community Prevalence`,
             y = Med,
             ymin = q25,
             ymax = q75,
             shape = `Test Strategy`,
             col = `Test Frequency`)) +
    geom_hline(yintercept = 20, col = "grey50", alpha = 0.5) +
    geom_point(position = position_dodge(0.5)) +
    geom_errorbar(width = 0.1, position = position_dodge(0.5)) +
    theme_classic() +
    facet_grid(.~Rlab, scales = "free_y", switch = "y") +
    scale_y_continuous(breaks = c(0,20,40,60,80,100,200,300,400,500),
                       limits = c(0,450)) +
    scale_color_manual(values = c("darkred",
                                  "red",
                                  "#ff94b0",
                                  "#c9ca8c",
                                  "#008126")) +
    theme(strip.background = element_blank(),
          strip.text = element_text(face = "bold", size = 14),
          strip.placement = "outside",
          panel.border = element_rect(color = "grey50", fill = NA, size = 0.5),
          axis.title = element_text(size = 11),
          axis.text = element_text(size = 10)) +
    labs(y = expression(bold(I[" sim"]^" tot")))
```

An alternative threshold approach to aid decision-making, particularly in resource-constrained settings, is the ITER. Figure 3 shows estimates of the ITER across transmission scenarios only for systematic testing strategies since they were found to substantially outperform random strategies. In the highest transmission scenario ($\mathcal{R}=1.5$, $1\%$ community prevalence), testing on the first day of every other work week ($f=0.5$, fig 3 circles) leads to $ITER=$ `r round(sims_sum_ggplot %>% filter(measure == "ITER", work_sched == "leaky", testsys == "systematic", testfreq == 0.5, commprev == "1%", R == 1.5) %>% pull(Med), 2)` (IQR `r round(sims_sum_ggplot %>%  filter(measure == "ITER", work_sched == "leaky", testsys == "systematic", testfreq == 0.5, commprev == "1%", R == 1.5) %>% pull(q25), 2)` - `r round(sims_sum_ggplot %>%  filter(measure == "ITER", work_sched == "leaky", testsys == "systematic", testfreq == 0.5, commprev == "1%", R == 1.5) %>% pull(q75), 2)`), while increasing test frequency to weekly, $f=1$, results in $ITER=$ `r round(sims_sum_ggplot %>% filter(measure == "ITER", work_sched == "leaky", testsys == "systematic", testfreq == 1, commprev == "1%", R == 1.5) %>% pull(Med), 2)` (IQR `r round(sims_sum_ggplot %>%  filter(measure == "ITER", work_sched == "leaky", testsys == "systematic", testfreq == 1, commprev == "1%", R == 1.5) %>% pull(q25), 2)` - `r round(sims_sum_ggplot %>%  filter(measure == "ITER", work_sched == "leaky", testsys == "systematic", testfreq == 1, commprev == "1%", R == 1.5) %>% pull(q75), 2)`), to $f=2$: $ITER=$ `r round(sims_sum_ggplot %>% filter(measure == "ITER", work_sched == "leaky", testsys == "systematic", testfreq == 2, commprev == "1%", R == 1.5) %>% pull(Med), 2)` (IQR `r round(sims_sum_ggplot %>%  filter(measure == "ITER", work_sched == "leaky", testsys == "systematic", testfreq == 2, commprev == "1%", R == 1.5) %>% pull(q25), 2)` - `r round(sims_sum_ggplot %>%  filter(measure == "ITER", work_sched == "leaky", testsys == "systematic", testfreq == 2, commprev == "1%", R == 1.5) %>% pull(q75), 2)`), and to $f=4$: $ITER=$ `r round(sims_sum_ggplot %>% filter(measure == "ITER", work_sched == "leaky", testsys == "systematic", testfreq == 4, commprev == "1%", R == 1.5) %>% pull(Med), 2)` (IQR `r round(sims_sum_ggplot %>%  filter(measure == "ITER", work_sched == "leaky", testsys == "systematic", testfreq == 4, commprev == "1%", R == 1.5) %>% pull(q25), 2)` - `r round(sims_sum_ggplot %>%  filter(measure == "ITER", work_sched == "leaky", testsys == "systematic", testfreq == 4, commprev == "1%", R == 1.5) %>% pull(q75), 2)`). These values approximately correspond to test positivity rates of `r paste0(round(100*1/sims_sum_ggplot %>% filter(measure == "ITER", work_sched == "leaky", testsys == "systematic", testfreq == 0.5, commprev == "1%", R == 1.5) %>% pull(Med), 2), "%")`, `r paste0(round(100*1/sims_sum_ggplot %>% filter(measure == "ITER", work_sched == "leaky", testsys == "systematic", testfreq == 1, commprev == "1%", R == 1.5) %>% pull(Med), 2), "%")`, `r paste0(round(100*1/sims_sum_ggplot %>% filter(measure == "ITER", work_sched == "leaky", testsys == "systematic", testfreq == 2, commprev == "1%", R == 1.5) %>% pull(Med), 2), "%")`, and `r paste0(round(100*1/sims_sum_ggplot %>% filter(measure == "ITER", work_sched == "leaky", testsys == "systematic", testfreq == 4, commprev == "1%", R == 1.5) %>% pull(Med), 2), "%")` due to the interpretation of the ITER as the number of tests per positive result. Figure 3 also provides an example reference line at $ITER=400$, corresponding to an approximate $0.25\%$ test positivity, to demonstrate how testing frequency may be determined from the transmission scenario and target ITER, which may be influenced by the number of tests available.

```{r sim_results_iter, include = TRUE, fig.width=5, fig.height=4, fig.cap="**Figure 3. Incremental test effectiveness ratio (ITER) from simulations implementing systematic testing across transmission scenarios and testing frequencies**. The ITER remains relatively low in higher transmission scenarios even at high ($f=4$) testing frequencies, potentially favoring such high-frequency testing strategies when within-facility transmission ($\\mathcal{R}$) and/or community prevalence are high. The y-axis is log-transformed and the horizontal line at $ITER=400$ is provided to aid visual comparison across scenarios. Error bars represent the interquartile range of expected infections derived from 100 simulations per scenario."}
sims_sum_ggplot %>% 
  filter(measure == "ITER", work_sched == "leaky", testsys == "systematic", testfreq > 0) %>% 
  ggplot(aes(x = `Community Prevalence`,
             y = Med,
             ymin = q25,
             ymax = q75,
             col = as.factor(R),
             shape = `Test Frequency`)) +
  geom_point(size = 1.2, position = position_dodge(0.5)) +
  geom_errorbar(width = 0.1, position = position_dodge(0.5)) +
  geom_hline(yintercept = 100, col = "grey50", alpha = 0.5) +
  scale_y_continuous(trans = "log", 
                     breaks = c(25,100, 400, 800, 1600, 3200, 6400)) +
  scale_color_manual(values = c("#f68f46ff",
                                "darkred",
                                "#7e4e90ff")) +
  theme_classic() +
  theme(axis.title = element_text(size = 12),
        axis.text = element_text(size = 10)) +
  labs(y = "ITER",
       col = expression(italic(R)))

```

### "Grid search" Over frequency, delay, and systematic vs random testing  
Basically interested in three variables: pcr vs antigen testing (which basically comes down to turnaround time if we assume antigen tests are equal to pcr in their ability to detect active infection), random vs systematic day of testing, and frequency of testing. We propose the following scenarios encompassing combinations of these variables to explore:

* **S1)** No testing  
* **S2)** Random PCR testing once per work week with test report on second day following test  
* **S3)** Random antigen testing once per work week with immediate test report  
* **S4)** PCR testing on first day of work week with test report on second day following test  
* **S5)** Antigen testing on first day of work week with immediate test report  
* **S6)** Random PCR testing twice per work week with test report on second day following test  
* **S7)** Random antigen testing twice per work week with immediate test report  
* **S8)** PCR testing on first and third day of work week with test report on second day following test  
* **S9)** Antigen testing on first and third day of work week with immediate test report  
* **S10)** PCR testing on all days of work week with test report on second day following test  
* **S11)** Antigen testing on all days of work week with immediate test report  

### PCR test turnaround time and antigen test sensitivity  
```{r tat_sens_grid, cache = T, eval = FALSE}
tat_sens_grid <- expand.grid(delay = c(0,c(1:3)/dt-1),
                             test_sens = c(0, 0.025, 0.05, 0.1))

tat_sens_grid <- as.data.frame(tat_sens_grid) %>% 
  slice(rep(1:n(), each = n_sims))

test_day1_tat_sens_comp <- apply(tat_sens_grid, 1, function(x){
  cases_tests <- sim_work_transmission(Lambda = lambda2*dt,
                                       R_work   = 0,
                                       R       = 1,
                                       delay   = x[1],
                                       test_sens = x[2],
                                       workers = workers_testday1,
                                       sim_t   = sim_t,
                                       dt      = dt,
                                       verbose = F)$cases_tests
  
  return(c("tot_cases" = sum(cases_tests$exp_cases),
           "tot_tests" = sum(cases_tests$tests_adm)))
})

tat_sens_res <- cbind(tat_sens_grid, t(test_day1_tat_sens_comp)) %>% 
  group_by(delay, test_sens) %>% 
  summarise(across(.cols = c("tot_cases", "tot_tests"),
                   .fns  = list("median", "q_025", "q_25", "q_75", "q_975"))) %>% 
  ungroup()

```

```{r tat_sens_grid_plot, include = TRUE, eval = FALSE}
tat_sens_res %>% 
  ggplot(aes(x = as.factor(test_sens), y = as.factor(delay), fill = tot_cases_1)) +
    geom_tile() +
    theme_bw() +
    scale_y_continuous(labels = c("0", "1", "2", "3")) +
    scale_fill_viridis_b() +
    labs(x = "Test Detection limit",
         y = "Test Delay",
         fill = "Expected\nCases")

```

